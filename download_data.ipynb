{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in e:\\anaconda3\\lib\\site-packages (1.6.17)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in e:\\anaconda3\\lib\\site-packages (from kaggle) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda3\\lib\\site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: python-slugify in e:\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in e:\\anaconda3\\lib\\site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: bleach in e:\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from bleach->kaggle) (24.2)\n",
      "Requirement already satisfied: webencodings in e:\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in e:\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kaggle) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must ensure that you have the file kaggle.json in your ~/.kaggle directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded home-data-for-ml-course to Lab06a\n",
      "Downloaded titanic to Lab06b\n",
      "Downloaded digit-recognizer to Lab08\n",
      "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
      "Downloaded lakshmi25npathi/imdb-dataset-of-50k-movie-reviews to Lab09a\n",
      "Dataset URL: https://www.kaggle.com/datasets/shoumikgoswami/annotated-gmb-corpus\n",
      "Downloaded shoumikgoswami/annotated-gmb-corpus to Lab09b\n",
      "Dataset URL: https://www.kaggle.com/datasets/tuannguyenvananh/iwslt15-englishvietnamese\n",
      "Downloaded tuannguyenvananh/iwslt15-englishvietnamese to Lab10a\n",
      "Dataset URL: https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail\n",
      "Downloaded gowrishankarp/newspaper-text-summarization-cnn-dailymail to Lab10b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:00<00:00, 395875.88 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1801350/1801350 [00:05<00:00, 327821.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<00:00, 268508.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Salesforce/wikitext to Lab11\\wikitext-dataset\n",
      "All datasets have been downloaded and extracted to their respective directories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kaggle\n",
    "from datasets import load_dataset\n",
    "import zipfile\n",
    "\n",
    "def create_directory(lab_name):\n",
    "    \"\"\"Create a directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(lab_name, exist_ok=True)\n",
    "\n",
    "def is_file_downloaded(file_path):\n",
    "    \"\"\"Check if a file has already been downloaded.\"\"\"\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "def extract_zip(file_path, extract_to):\n",
    "    \"\"\"Extract zip files to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def download_kaggle_competition(competition_name, lab_name):\n",
    "    \"\"\"Download Kaggle competition files.\"\"\"\n",
    "    create_directory(lab_name)\n",
    "    file_path = os.path.join(lab_name, f\"{competition_name}.zip\")\n",
    "    if not is_file_downloaded(file_path):\n",
    "        kaggle.api.competition_download_files(competition_name, path=lab_name)\n",
    "        print(f\"Downloaded {competition_name} to {lab_name}\")\n",
    "    else:\n",
    "        print(f\"{competition_name} already downloaded.\")\n",
    "    \n",
    "    extract_zip(file_path, lab_name)\n",
    "\n",
    "def download_kaggle_dataset(dataset_name, lab_name):\n",
    "    \"\"\"Download Kaggle dataset files.\"\"\"\n",
    "    create_directory(lab_name)\n",
    "    dataset_slug = dataset_name.split('/')[-1]\n",
    "    file_path = os.path.join(lab_name, f\"{dataset_slug}.zip\")\n",
    "    if not is_file_downloaded(file_path):\n",
    "        kaggle.api.dataset_download_files(dataset_name, path=lab_name)\n",
    "        print(f\"Downloaded {dataset_name} to {lab_name}\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} already downloaded.\")\n",
    "    \n",
    "    extract_zip(file_path, lab_name)\n",
    "\n",
    "def download_huggingface_dataset(dataset_name, config, lab_name):\n",
    "    \"\"\"Download Hugging Face datasets.\"\"\"\n",
    "    create_directory(lab_name)\n",
    "    dataset_dir = os.path.join(lab_name, f\"{dataset_name.split('/')[-1]}-dataset\")\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        dataset = load_dataset(dataset_name, config)\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        print(f\"Downloaded {dataset_name} to {dataset_dir}\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} already downloaded.\")\n",
    "\n",
    "# Kaggle Competitions\n",
    "download_kaggle_competition('home-data-for-ml-course', 'Lab06a')\n",
    "download_kaggle_competition('titanic', 'Lab06b')\n",
    "download_kaggle_competition('digit-recognizer', 'Lab08')\n",
    "\n",
    "# Kaggle Datasets\n",
    "download_kaggle_dataset('lakshmi25npathi/imdb-dataset-of-50k-movie-reviews', 'Lab09a')\n",
    "download_kaggle_dataset('shoumikgoswami/annotated-gmb-corpus', 'Lab09b')\n",
    "download_kaggle_dataset('tuannguyenvananh/iwslt15-englishvietnamese', 'Lab10a')\n",
    "download_kaggle_dataset('gowrishankarp/newspaper-text-summarization-cnn-dailymail', 'Lab10b')\n",
    "\n",
    "# Hugging Face Dataset\n",
    "download_huggingface_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\", \"Lab11\")\n",
    "\n",
    "print(\"All datasets have been downloaded and extracted to their respective directories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure of: e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\NLP_Data\n",
      "\n",
      "ðŸ“ .git\n",
      "  ðŸ“„ HEAD\n",
      "  ðŸ“„ config\n",
      "  ðŸ“„ description\n",
      "  ðŸ“ fsmonitor--daemon\n",
      "    ðŸ“ cookies\n",
      "  ðŸ“ hooks\n",
      "    ðŸ“„ applypatch-msg.sample\n",
      "    ðŸ“„ commit-msg.sample\n",
      "    ðŸ“„ fsmonitor-watchman.sample\n",
      "    ðŸ“„ post-update.sample\n",
      "    ðŸ“„ pre-applypatch.sample\n",
      "    ðŸ“„ pre-commit.sample\n",
      "    ðŸ“„ pre-merge-commit.sample\n",
      "    ðŸ“„ pre-push.sample\n",
      "    ðŸ“„ pre-rebase.sample\n",
      "    ðŸ“„ pre-receive.sample\n",
      "    ðŸ“„ prepare-commit-msg.sample\n",
      "    ðŸ“„ push-to-checkout.sample\n",
      "    ðŸ“„ sendemail-validate.sample\n",
      "    ðŸ“„ update.sample\n",
      "  ðŸ“ info\n",
      "    ðŸ“„ exclude\n",
      "  ðŸ“ objects\n",
      "    ðŸ“ info\n",
      "    ðŸ“ pack\n",
      "  ðŸ“ refs\n",
      "    ðŸ“ heads\n",
      "    ðŸ“ tags\n",
      "ðŸ“ Lab06a\n",
      "  ðŸ“„ data_description.txt\n",
      "  ðŸ“„ home-data-for-ml-course.zip\n",
      "  ðŸ“„ sample_submission.csv\n",
      "  ðŸ“„ sample_submission.csv.gz\n",
      "  ðŸ“„ test.csv\n",
      "  ðŸ“„ test.csv.gz\n",
      "  ðŸ“„ train.csv\n",
      "  ðŸ“„ train.csv.gz\n",
      "ðŸ“ Lab06b\n",
      "  ðŸ“„ gender_submission.csv\n",
      "  ðŸ“„ test.csv\n",
      "  ðŸ“„ titanic.zip\n",
      "  ðŸ“„ train.csv\n",
      "ðŸ“ Lab08\n",
      "  ðŸ“„ digit-recognizer.zip\n",
      "  ðŸ“„ sample_submission.csv\n",
      "  ðŸ“„ test.csv\n",
      "  ðŸ“„ train.csv\n",
      "ðŸ“ Lab09a\n",
      "  ðŸ“„ IMDB Dataset.csv\n",
      "  ðŸ“„ imdb-dataset-of-50k-movie-reviews.zip\n",
      "ðŸ“ Lab09b\n",
      "  ðŸ“„ GMB_dataset.txt\n",
      "  ðŸ“„ annotated-gmb-corpus.zip\n",
      "ðŸ“ Lab10a\n",
      "  ðŸ“ IWSLT'15 en-vi\n",
      "    ðŸ“„ dict.en-vi.txt\n",
      "    ðŸ“„ luong-manning-iwslt15.pdf\n",
      "    ðŸ“„ train.en.txt\n",
      "    ðŸ“„ train.vi.txt\n",
      "    ðŸ“„ tst2012.en.txt\n",
      "    ðŸ“„ tst2012.vi.txt\n",
      "    ðŸ“„ tst2013.en.txt\n",
      "    ðŸ“„ tst2013.vi.txt\n",
      "    ðŸ“„ vocab.en.txt\n",
      "    ðŸ“„ vocab.vi.txt\n",
      "  ðŸ“„ iwslt15-englishvietnamese.zip\n",
      "ðŸ“ Lab10b\n",
      "  ðŸ“ cnn_dailymail\n",
      "    ðŸ“„ test.csv\n",
      "    ðŸ“„ train.csv\n",
      "    ðŸ“„ validation.csv\n",
      "  ðŸ“„ newspaper-text-summarization-cnn-dailymail.zip\n",
      "ðŸ“ Lab11\n",
      "  ðŸ“ wikitext-dataset\n",
      "    ðŸ“„ dataset_dict.json\n",
      "    ðŸ“ test\n",
      "      ðŸ“„ data-00000-of-00001.arrow\n",
      "      ðŸ“„ dataset_info.json\n",
      "      ðŸ“„ state.json\n",
      "    ðŸ“ train\n",
      "      ðŸ“„ data-00000-of-00002.arrow\n",
      "      ðŸ“„ data-00001-of-00002.arrow\n",
      "      ðŸ“„ dataset_info.json\n",
      "      ðŸ“„ state.json\n",
      "    ðŸ“ validation\n",
      "      ðŸ“„ data-00000-of-00001.arrow\n",
      "      ðŸ“„ dataset_info.json\n",
      "      ðŸ“„ state.json\n",
      "ðŸ“„ download_data.ipynb\n"
     ]
    }
   ],
   "source": [
    "def print_directory_structure(start_path, indent=\"\"):\n",
    "    \"\"\"Recursively prints the directory structure.\"\"\"\n",
    "    items = sorted(os.listdir(start_path))\n",
    "    for item in items:\n",
    "        item_path = os.path.join(start_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{indent}ðŸ“ {item}\")\n",
    "            print_directory_structure(item_path, indent + \"  \")\n",
    "        else:\n",
    "            print(f\"{indent}ðŸ“„ {item}\")\n",
    "\n",
    "# Set the starting path to your current directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Directory structure of: {current_directory}\\n\")\n",
    "print_directory_structure(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory storage of 'e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\NLP_Data': 2605.12 MB\n"
     ]
    }
   ],
   "source": [
    "def get_directory_size(start_path='.'):\n",
    "    \"\"\"Calculate the total size of a directory and its subdirectories.\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # Add the size of the file to total_size\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "# Get the size of the current directory\n",
    "current_directory = os.getcwd()\n",
    "directory_size = get_directory_size(current_directory)\n",
    "\n",
    "# Convert size to MB\n",
    "size_in_mb = directory_size / (1024 * 1024)\n",
    "print(f\"Total memory storage of '{current_directory}': {size_in_mb:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
