{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Language Model and Application for Spelling Error Correction**\n",
    "\n",
    "**Objective**: Develop a simple English syntax error correction program.\n",
    "\n",
    "a) Build a language model based on n-grams using the Laplace smoothing method for the following models:\n",
    "\n",
    "- 1-gram\n",
    "- 2-gram\n",
    "- 3-gram\n",
    "\n",
    "b) Calculate the probability of a sentence and compute the Perplexity of a sentence based on 1-gram, 2-gram, and 3-gram models.\n",
    "\n",
    "c) Analyze the results (Provide your own examples of spelling errors and calculate the probability of two similar sentences, where one has the correct word order and the other has an incorrect word order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\python312\\lib\\site-packages (1.9.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\python312\\lib\\site-packages (from wordcloud) (2.1.3)\n",
      "Requirement already satisfied: pillow in c:\\python312\\lib\\site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\python312\\lib\\site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python312\\lib\\site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python312\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python312\\lib\\site-packages (from matplotlib->wordcloud) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python312\\lib\\site-packages (from matplotlib->wordcloud) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python312\\lib\\site-packages (from matplotlib->wordcloud) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable being used: c:\\Python312\\python.exe\n",
      "Python version: 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n",
      "Operating System: Windows\n",
      "OS Version: 10.0.19045\n",
      "OS Release: 10\n",
      "Machine: AMD64\n",
      "Running in Visual Studio Code\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# import platform\n",
    "\n",
    "# # Python environment details\n",
    "# print(\"Python executable being used:\", sys.executable)\n",
    "# print(\"Python version:\", sys.version)\n",
    "\n",
    "# # Operating System details\n",
    "# print(\"Operating System:\", platform.system())\n",
    "# print(\"OS Version:\", platform.version())\n",
    "# print(\"OS Release:\", platform.release())\n",
    "\n",
    "# # Machine and architecture details\n",
    "# print(\"Machine:\", platform.machine())\n",
    "\n",
    "# # Visual Studio Code details (based on environment variable)\n",
    "# vscode_info = os.environ.get('VSCODE_PID', None)\n",
    "# if vscode_info:\n",
    "#     print(\"Running in Visual Studio Code\")\n",
    "# else:\n",
    "#     print(\"Not running in Visual Studio Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load and preprocess text data from a file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        sentences = file.readlines()\n",
    "    \n",
    "    # Add <s> and </s> to each sentence and tokenize\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip().lower()\n",
    "        if sentence:  \n",
    "            tokens.extend([\"<s>\"] + sentence.split() + [\"</s>\"])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for building a specific n-gram model with given corpus and n value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_model(corpus, n):\n",
    "    \"\"\"\n",
    "    Build an n-gram model from a given corpus and given n\n",
    "    \"\"\"\n",
    "    ngram_counts = defaultdict(int)\n",
    "    n_minus_1_counts = defaultdict(int)\n",
    "    vocabulary = set(corpus)\n",
    "\n",
    "    for i in range(len(corpus) - n + 1):\n",
    "        ngram = tuple(corpus[i:i + n])\n",
    "        n_minus_1_gram = tuple(corpus[i:i + n - 1])\n",
    "        ngram_counts[ngram] += 1\n",
    "        n_minus_1_counts[n_minus_1_gram] += 1\n",
    "\n",
    "    return ngram_counts, n_minus_1_counts, len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for computing sentence probabilities with laplace smoothing\n",
    "\n",
    "1. **Understanding N-gram Probabilities**:\n",
    "   - An n-gram is a sequence of `n` tokens (words).\n",
    "   - The probability of an n-gram with Laplace smoothing is computed as:\n",
    "   - \n",
    "     $\n",
    "     P(w_i | w_{i-(n-1)}, \\ldots, w_{i-1}) = \\frac{\\text{Count}(w_{i-(n-1)}, \\ldots, w_i) + 1}{\\text{Count}(w_{i-(n-1)}, \\ldots, w_{i-1}) + V}\n",
    "     $\n",
    "\n",
    "     Where:\n",
    "     - $\\text{Count}(w_{i-(n-1)}, \\ldots, w_i)$: Count of the n-gram in the training data.\n",
    "     - $\\text{Count}(w_{i-(n-1)}, \\ldots, w_{i-1})$: Count of the (n-1)-gram prefix.\n",
    "     - $V$: Vocabulary size (total number of unique tokens in the training data).\n",
    "\n",
    "2. **Handling Zero Counts**:\n",
    "   - If the n-gram or its prefix does not appear in the training data, the smoothing process ensures a non-zero probability by adding 1 to the numerator and the vocabulary size $V$ to the denominator.\n",
    "\n",
    "3. **Sentence Probability**:\n",
    "   - A sentence's probability is the product of probabilities for all n-grams in the sentence:\n",
    "   - \n",
    "     $\n",
    "     P(\\text{sentence}) = \\prod_{i=1}^{N} P(w_i | w_{i-(n-1)}, \\ldots, w_{i-1})\n",
    "     $\n",
    "\n",
    "     Here, the sentence is tokenized with start (`<s>`) and end (`</s>`) markers to capture context at the boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplace_probability(ngram, ngram_counts, n_minus_1_counts, vocab_size):\n",
    "    \"\"\"\n",
    "    Compute the Laplace smoothed probability of an n-gram.\n",
    "    \"\"\"\n",
    "    ngram_count = ngram_counts[ngram]\n",
    "    n_minus_1_count = n_minus_1_counts[ngram[:-1]] if len(ngram) > 1 else sum(ngram_counts.values())\n",
    "    return (ngram_count + 1) / (n_minus_1_count + vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_probability(sentence, ngram_counts, n_minus_1_counts, vocab_size, n):\n",
    "    \"\"\"\n",
    "    Compute the probability of a sentence using an n-gram model with laplace smoothing\n",
    "    \"\"\"\n",
    "    tokens = [\"<s>\"] + sentence.split() + [\"</s>\"]\n",
    "    prob = 1.0\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i + n])\n",
    "        prob *= compute_laplace_probability(ngram, ngram_counts, n_minus_1_counts, vocab_size)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for computing perplexity with the used of laplace probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sentence, ngram_counts, n_minus_1_counts, vocab_size, n):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a sentence using an n-gram model.\n",
    "    \"\"\"\n",
    "    tokens = [\"<s>\"] + sentence.split() + [\"</s>\"]\n",
    "    prob = 0.0\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i + n])\n",
    "        prob += math.log(compute_laplace_probability(ngram, ngram_counts, n_minus_1_counts, vocab_size))\n",
    "    prob = -prob / len(tokens)\n",
    "    return math.exp(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for building first 3 n-gram models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_models(filepath):\n",
    "    \"\"\"\n",
    "    Build n-gram models for 1-gram, 2-gram, and 3-gram.\n",
    "    \"\"\"\n",
    "    corpus = load_data(filepath)\n",
    "    ngram_models = {}\n",
    "    \n",
    "    for n in range(1, 4): \n",
    "        ngram_counts, n_minus_1_counts, vocab_size = build_ngram_model(corpus, n)\n",
    "        ngram_models[n] = (ngram_counts, n_minus_1_counts, vocab_size)\n",
    "    \n",
    "    return ngram_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for calculating the probability and the perplexity of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentence(sentence, ngram_models):\n",
    "    \"\"\"\n",
    "    Analyze a sentence by computing its probability and perplexity for 1-gram, 2-gram, and 3-gram models.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for n, (ngram_counts, n_minus_1_counts, vocab_size) in ngram_models.items():\n",
    "        prob = sentence_probability(sentence, ngram_counts, n_minus_1_counts, vocab_size, n)\n",
    "        perplexity = compute_perplexity(sentence, ngram_counts, n_minus_1_counts, vocab_size, n)\n",
    "        results[n] = {\"probability\": prob, \"perplexity\": perplexity}\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for comparing 2 sentences by analyzing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sentences(correct_sentence, incorrect_sentence, ngram_models):\n",
    "    \"\"\"\n",
    "    Compare probabilities and perplexities of two sentences: one correct and one incorrect.\n",
    "    \"\"\"\n",
    "    correct_results = analyze_sentence(correct_sentence, ngram_models)\n",
    "    incorrect_results = analyze_sentence(incorrect_sentence, ngram_models)\n",
    "\n",
    "    comparison = {}\n",
    "    for n in correct_results.keys():\n",
    "        # Extract probabilities and perplexities for both sentences\n",
    "        correct_prob = correct_results[n][\"probability\"]\n",
    "        incorrect_prob = incorrect_results[n][\"probability\"]\n",
    "        correct_perplexity = correct_results[n][\"perplexity\"]\n",
    "        incorrect_perplexity = incorrect_results[n][\"perplexity\"]\n",
    "\n",
    "        if correct_prob > incorrect_prob:\n",
    "            higher_sentence = \"Correct Sentence\"\n",
    "        elif incorrect_prob > correct_prob:\n",
    "            higher_sentence = \"Incorrect Sentence\"\n",
    "        else:\n",
    "            higher_sentence = \"Both sentences have equal probability\"\n",
    "\n",
    "        # Store comparison results\n",
    "        comparison[n] = {\n",
    "            \"correct\": {\n",
    "                \"probability\": correct_prob,\n",
    "                \"perplexity\": correct_perplexity,\n",
    "            },\n",
    "            \"incorrect\": {\n",
    "                \"probability\": incorrect_prob,\n",
    "                \"perplexity\": incorrect_perplexity,\n",
    "            },\n",
    "            \"higher_probability\": higher_sentence,\n",
    "            \"probability_difference\": abs(correct_prob - incorrect_prob),\n",
    "        }\n",
    "\n",
    "    return comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Main functions for the Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Develop a simple English syntax error correction program.\n",
    "\n",
    "a) Build a language model based on n-grams using the Laplace smoothing method for the following models:\n",
    "\n",
    "- 1-gram\n",
    "- 2-gram\n",
    "- 3-gram\n",
    "\n",
    "b) Calculate the probability of a sentence and compute the Perplexity of a sentence based on 1-gram, 2-gram, and 3-gram models.\n",
    "\n",
    "c) Analyze the results (Provide your own examples of spelling errors and calculate the probability of two similar sentences, where one has the correct word order and the other has an incorrect word order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building n-gram models...\n",
      "\n",
      "Comparison of Correct vs Incorrect Sentences:\n",
      "\n",
      "1-gram Model:\n",
      "Correct Sentence - Probability: 5.360671e-27, Perplexity: 1922.42\n",
      "Incorrect Sentence - Probability: 5.360671e-27, Perplexity: 1922.42\n",
      "Higher Probability: Correct Sentence\n",
      "Probability Difference: 7.174648e-43\n",
      "\n",
      "2-gram Model:\n",
      "Correct Sentence - Probability: 9.049810e-29, Perplexity: 3201.99\n",
      "Incorrect Sentence - Probability: 9.049810e-29, Perplexity: 3201.99\n",
      "Higher Probability: Incorrect Sentence\n",
      "Probability Difference: 1.121039e-44\n",
      "\n",
      "3-gram Model:\n",
      "Correct Sentence - Probability: 4.723293e-31, Perplexity: 6176.17\n",
      "Incorrect Sentence - Probability: 4.723293e-31, Perplexity: 6176.17\n",
      "Higher Probability: Correct Sentence\n",
      "Probability Difference: 8.758115e-47\n"
     ]
    }
   ],
   "source": [
    "# Path \n",
    "dataset_path = os.path.join(os.getcwd(), \"tedtalk.txt\")\n",
    "\n",
    "# n-gram models\n",
    "print(\"Building n-gram models...\")\n",
    "ngram_models = build_ngram_models(dataset_path)\n",
    "\n",
    "# Example sentences for comparison\n",
    "correct_sentence_example = \"the cat sat on the mat\"  \n",
    "incorrect_sentence = \"the mat sat on the cat\"\n",
    "\n",
    "# Compare probabilities and perplexities\n",
    "comparison = compare_sentences(correct_sentence_example, incorrect_sentence, ngram_models)\n",
    "\n",
    "# Display comparison results\n",
    "print(\"\\nComparison of Correct vs Incorrect Sentences:\")\n",
    "for n, results in comparison.items():\n",
    "    print(f\"\\n{n}-gram Model:\")\n",
    "    print(f\"Correct Sentence - Probability: {results['correct']['probability']:.6e}, Perplexity: {results['correct']['perplexity']:.2f}\")\n",
    "    print(f\"Incorrect Sentence - Probability: {results['incorrect']['probability']:.6e}, Perplexity: {results['incorrect']['perplexity']:.2f}\")\n",
    "    print(f\"Higher Probability: {results['higher_probability']}\")\n",
    "    print(f\"Probability Difference: {results['probability_difference']:.6e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
