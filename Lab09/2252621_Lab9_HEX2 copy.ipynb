{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Exercise on Named Entity Recognition\n",
    "\n",
    "Implement a **Recurrent Neural Network model** (**[Bidirectional LSTM-CRF Models for Sequence Tagging](https://arxiv.org/pdf/1508.01991)**) to extract named entities from text. Entity labels are encoded using the **BIO notation**, where each entity label is assigned a **B** (Beginning) or **I** (Inside) tag. The **B-** tag indicates the beginning of an entity, while the **I-** tag marks words inside the same entity.\n",
    "\n",
    "These tags help identify multi-word entities. For example, in the phrase **\"World War II\"**, the labels would be: **(B-eve, I-eve, I-eve)**. Words that do not belong to any entity are labeled as **O** (Outside).\n",
    "\n",
    "- **Data**: [Annotated GMB Corpus](https://www.kaggle.com/datasets/shoumikgoswami/annotated-gmb-corpus?select=GMB_dataset.txt) *(the last 10% of rows serve as the test set).*\n",
    "\n",
    "**Note**: Submit only a **single Jupyter Notebook file** that can handle all tasks, including data downloading, preprocessing, model training, and model evaluation. *(Submissions that do not follow the guidelines will receive a score of 0.)*\n",
    "\n",
    "## Grading Criteria\n",
    "\n",
    "For valid submissions, scores will be assigned based on the **leaderboard ranking** (**strictly greater**):\n",
    "\n",
    "- **Top 25%** → **10 points**\n",
    "- **25% - 50%** → **9.0 points**\n",
    "- **50% - 75%** → **8.0 points**\n",
    "- **75% - 100%** → **7.0 points**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas gdown tensorflow keras-crf matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, TimeDistributed, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_crf import CRF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/drive/folders/1K1sEol_XTBWkcUwU7Yhi42--L0FyviMK?usp=sharing\n",
    "\n",
    "# URL of the shared folder\n",
    "folder_url = \"https://drive.google.com/drive/folders/1K1sEol_XTBWkcUwU7Yhi42--L0FyviMK?usp=sharing\"\n",
    "\n",
    "# Output directory where the folder will be saved\n",
    "output_dir = \"./NLP_Data_GDrive\"\n",
    "\n",
    "# Download the folder\n",
    "gdown.download_folder(folder_url, output=output_dir, quiet=False, use_cookies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (Ensure file path is correct)\n",
    "file_path = os.path.join(output_dir, \"GMB_dataset.txt\")  # Adjust file path if needed\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\"File not found. Check the download process and file paths.\")\n",
    "\n",
    "df = pd.read_csv(file_path, delimiter=\"\\t\", names=[\"Sentence#\", \"Word\", \"POS\", \"Tag\"], skiprows=1)\n",
    "\n",
    "# Fill missing Sentence# values\n",
    "df[\"Sentence#\"] = df[\"Sentence#\"].fillna(method=\"ffill\")\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group words by sentences\n",
    "sentences = df.groupby(\"Sentence#\")[\"Word\"].apply(list).values\n",
    "tags = df.groupby(\"Sentence#\")[\"Tag\"].apply(list).values\n",
    "\n",
    "# Create a vocabulary and tag index\n",
    "words = list(set(df[\"Word\"].values))\n",
    "words.append(\"PAD\")  # Add padding token\n",
    "n_words = len(words)\n",
    "\n",
    "tags_set = list(set(df[\"Tag\"].values))\n",
    "n_tags = len(tags_set)\n",
    "\n",
    "# Word-to-index and index-to-word mappings\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Tag-to-index and index-to-tag mappings\n",
    "tag2idx = {t: i for i, t in enumerate(tags_set)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {n_words}, Number of Tags: {n_tags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50  # Adjust as needed\n",
    "\n",
    "# Convert words to indices and pad sequences\n",
    "X = [[word2idx[w] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# Convert tags to indices and pad sequences\n",
    "y = [[tag2idx[t] for t in s] for s in tags]\n",
    "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "# Train-test split (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training Samples: {len(X_train)}, Testing Samples: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100  # Embedding size\n",
    "\n",
    "# Define Model\n",
    "input_layer = Input(shape=(MAX_LEN,))\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = Embedding(input_dim=n_words, output_dim=EMBEDDING_DIM, input_length=MAX_LEN)(input_layer)\n",
    "\n",
    "# BiLSTM Layer\n",
    "bi_lstm = Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.5, recurrent_dropout=0.25))(embedding)\n",
    "\n",
    "# TimeDistributed Dense Layer\n",
    "dense = TimeDistributed(Dense(n_tags, activation=\"relu\"))(bi_lstm)\n",
    "\n",
    "# CRF Layer\n",
    "crf = CRF(n_tags)\n",
    "output_layer = crf(dense)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, np.array(y_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to tag labels\n",
    "pred_tags = [[idx2tag[np.argmax(tag)] for tag in sentence] for sentence in test_pred]\n",
    "true_tags = [[idx2tag[np.argmax(tag)] for tag in sentence] for sentence in y_test]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean([1 if pred == true else 0 for p, t in zip(pred_tags, true_tags) for pred, true in zip(p, t)])\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
