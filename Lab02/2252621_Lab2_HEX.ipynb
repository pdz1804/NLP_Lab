{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFD7DVZ-xKdT"
   },
   "source": [
    "# Homework Lab 2: Text Preprocessing with Vietnamese\n",
    "**Overview:** In this exercise, we will build a text preprocessing program for Vietnamese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOAeiqdrxKdt"
   },
   "source": [
    "Import the necessary libraries. Note that we are using the underthesea library for Vietnamese tokenization. To install it, follow the instructions below. ([link](https://github.com/undertheseanlp/underthesea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: maturin in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pip in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (24.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (75.8.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: underthesea in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.8.4)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (8.1.7)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (0.9.10)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (2.32.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (1.5.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (6.0.2)\n",
      "Requirement already satisfied: underthesea-core==1.0.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from underthesea) (1.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from Click>=6.0->underthesea) (0.4.6)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->underthesea) (2024.7.24)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->underthesea) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->underthesea) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->underthesea) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->underthesea) (2024.8.30)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->underthesea) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->underthesea) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->underthesea) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "%pip install maturin\n",
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip install underthesea --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RrFQ_Ht_xKdu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import codecs\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I do this exercise on my visual studio code\n",
    "- So i need to clone the 2 repo to my working directory to use them\n",
    "- Then for the VNTC repo, I need to extract manually by hand these 2 files\n",
    "  - Lab02\\VNTC\\Data\\10Topics\\Ver1.1\\Test_Full.rar\n",
    "  - Lab02\\VNTC\\Data\\10Topics\\Ver1.1\\Train_Full.rar\n",
    "- Here is the relative path from the working directory to the files that will be used in this exercise after cloning the 2 repo and extract \n",
    "  - Lab02\\VNTC\\Data\\10Topics\\Ver1.1\\Test_Full\n",
    "  - Lab02\\VNTC\\Data\\10Topics\\Ver1.1\\Train_Full\n",
    "  - Lab02\\vietnamese-stopwords\\vietnamese-stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'VNTC' already exists and is not an empty directory.\n",
      "fatal: destination path 'vietnamese-stopwords' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/duyvuleo/VNTC.git\n",
    "!git clone https://github.com/stopwords/vietnamese-stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable being used: c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "Python version: 3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]\n",
      "Operating System: Windows\n",
      "OS Version: 10.0.19045\n",
      "OS Release: 10\n",
      "Machine: AMD64\n",
      "Running in Visual Studio Code\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# Python environment details\n",
    "print(\"Python executable being used:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "# Operating System details\n",
    "print(\"Operating System:\", platform.system())\n",
    "print(\"OS Version:\", platform.version())\n",
    "print(\"OS Release:\", platform.release())\n",
    "\n",
    "# Machine and architecture details\n",
    "print(\"Machine:\", platform.machine())\n",
    "\n",
    "# Visual Studio Code details (based on environment variable)\n",
    "vscode_info = os.environ.get('VSCODE_PID', None)\n",
    "if vscode_info:\n",
    "    print(\"Running in Visual Studio Code\")\n",
    "else:\n",
    "    print(\"Not running in Visual Studio Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC27lBQZxKdw"
   },
   "source": [
    "## Question 1: Create a Corpus and Survey the Data\n",
    "\n",
    "The data in this section is partially extracted from the [VNTC](https://github.com/duyvuleo/VNTC) dataset. VNTC is a Vietnamese news dataset covering various topics. In this section, we will only process the science topic from VNTC. We will create a corpus from both the train and test directories. Complete the following program:\n",
    "\n",
    "- Write `sentences_list` to a file named `dataset_name.txt`, with each element as a document on a separate line.\n",
    "- Check how many documents are in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GyNKT8wAxKdx",
    "outputId": "b2eb7c10-da8d-49cb-8b7d-4f6543700cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory contents: ['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh', 'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa', 'Vi tinh']\n",
      "Testing directory contents: ['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh', 'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa', 'Vi tinh']\n",
      "Train labels = Test labels\n",
      "Folder list (train/test): [['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh', 'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa', 'Vi tinh'], ['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh', 'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa', 'Vi tinh']]\n",
      "Processing folder: e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab02\\VNTC\\Data\\10Topics\\Ver1.1\\Train_Full\\Khoa hoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Khoa hoc: 100%|██████████| 1820/1820 [00:05<00:00, 305.11file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab02\\VNTC\\Data\\10Topics\\Ver1.1\\Test_Full\\Khoa hoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Khoa hoc: 100%|██████████| 2096/2096 [00:06<00:00, 332.88file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the corpus: 3916\n",
      "Corpus written to: VNTC_khoahoc.txt\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"VNTC_khoahoc\"\n",
    "# path = [\n",
    "#     r\"E:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab02\\VNTC\\Data\\10Topics\\Ver1.1\\Train_Full\",\n",
    "#     r\"E:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab02\\VNTC\\Data\\10Topics\\Ver1.1\\Test_Full\"\n",
    "# ]\n",
    "# Use relative paths\n",
    "path = [\n",
    "    os.path.join(os.getcwd(), \"VNTC\", \"Data\", \"10Topics\", \"Ver1.1\", \"Train_Full\"),\n",
    "    os.path.join(os.getcwd(), \"VNTC\", \"Data\", \"10Topics\", \"Ver1.1\", \"Test_Full\")\n",
    "]\n",
    "\n",
    "# Debug: Print the contents of each directory\n",
    "print(\"Training directory contents:\", os.listdir(path[0]))\n",
    "print(\"Testing directory contents:\", os.listdir(path[1]))\n",
    "\n",
    "if os.listdir(path[0]) == os.listdir(path[1]):\n",
    "    folder_list = [os.listdir(path[0]), os.listdir(path[1])]\n",
    "    print(\"Train labels = Test labels\")\n",
    "    print(\"Folder list (train/test):\", folder_list)\n",
    "else:\n",
    "    print(\"Train labels differ from test labels\")\n",
    "    folder_list = []\n",
    "    print(\"Folder list is empty:\", folder_list)\n",
    "\n",
    "doc_num = 0\n",
    "sentences_list = []\n",
    "\n",
    "for i in range(2):\n",
    "    for folder_name in folder_list[i]:\n",
    "        if folder_name != \"Khoa hoc\": \n",
    "            continue\n",
    "        \n",
    "        folder_path = path[i] + \"\\\\\" + folder_name\n",
    "        print(\"Processing folder:\", folder_path)\n",
    "        \n",
    "        # Use tqdm for file processing progress\n",
    "        for file_name in tqdm(glob.glob(os.path.join(folder_path, \"*.txt\")), desc=f\"Processing {folder_name}\", unit=\"file\"):\n",
    "            with codecs.open(file_name, \"r\", encoding=\"utf-16\") as f:\n",
    "                file_content = f.read().replace(\"\\r\\n\", \" \").strip()\n",
    "                sentences_list.append(file_content)\n",
    "            \n",
    "            doc_num += 1\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "# Write `sentences_list` to a file\n",
    "output_file = f\"{dataset_name}.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for sentence in sentences_list:\n",
    "        out_file.write(sentence + \"\\n\")\n",
    "\n",
    "# Display the number of documents in the corpus\n",
    "print(f\"Number of documents in the corpus: {doc_num}\")\n",
    "print(f\"Corpus written to: {output_file}\")\n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Write Preprocessing Functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KXHcDpuxKd0"
   },
   "source": [
    "### Question 2.1: Write a Function to Clean Text\n",
    "Hint:\n",
    "- The text should only retain the following characters: aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\\n",
    "- Then trim the whitespace in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "k8hIglDXxKd0"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    # Define the allowed characters (including Vietnamese-specific ones)\n",
    "    allowed_chars = \"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\\\'\"\n",
    "    \n",
    "    # Create a regex pattern that matches all allowed characters\n",
    "    pattern = f\"[^{allowed_chars}]\"  # Matches anything NOT in the allowed set\n",
    "\n",
    "    # Remove all unwanted characters\n",
    "    cleaned_string = re.sub(pattern, \" \", string)\n",
    "\n",
    "    # Trim excessive whitespace\n",
    "    cleaned_string = re.sub(r'\\s+', ' ', cleaned_string).strip()\n",
    "\n",
    "    return cleaned_string\n",
    "\n",
    "    #### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KfXstqAxKd1"
   },
   "source": [
    "### Question 2.2: Write a Function to Convert Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KRwgVjxhxKd1"
   },
   "outputs": [],
   "source": [
    "# make all text lowercase\n",
    "def text_lowercase(string):\n",
    "    #### YOUR CODE HERE ####\n",
    "    return string.lower()\n",
    "    #### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYM_GO_5xKd2"
   },
   "source": [
    "### Question 2.3: Tokenize Words\n",
    "Hint: Use the `word_tokenize()` function imported above with two parameters: `strings` and `format=\"text\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pty34NwyxKd2"
   },
   "outputs": [],
   "source": [
    "def tokenize(strings):\n",
    "    #### YOUR CODE HERE ####\n",
    "    tokens = word_tokenize(strings, format=\"text\")\n",
    "    return tokens\n",
    "    #### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gQGmL4gxKd2"
   },
   "source": [
    "### Question 2.4: Remove Stop Words\n",
    "To remove stop words, we use a list of Vietnamese stop words stored in the file `./vietnamese-stopwords.txt`. Complete the following program:\n",
    "- Check each word in the text (`strings`). If a word is not in the stop words list, add it to `doc_words`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aqStv2rPxKd3"
   },
   "outputs": [],
   "source": [
    "# relative to the vn stopwords dataset: Lab02\\vietnamese-stopwords\\vietnamese-stopwords.txt\n",
    "def remove_stopwords(strings):\n",
    "    # Define the path to the Vietnamese stop words file (relative path)\n",
    "    stopwords_path = os.path.join(os.getcwd(), \"vietnamese-stopwords\", \"vietnamese-stopwords.txt\")\n",
    "    \n",
    "    with open(stopwords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        stop_words = set(f.read().splitlines())\n",
    "    \n",
    "    words = strings.split()\n",
    "    doc_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(doc_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUNOKigIxKd4"
   },
   "source": [
    "## Question 2.5: Build a Preprocessing Function\n",
    "Hint: Call the functions `clean_str`, `text_lowercase`, `tokenize`, and `remove_stopwords` in order, then return the result from the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_vd-el91xKd_"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(strings):\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    # Step 1: Clean the string to remove unwanted characters\n",
    "    cleaned_text = clean_str(strings)\n",
    "    \n",
    "    # Step 2: Convert the text to lowercase\n",
    "    lowercase_text = text_lowercase(cleaned_text)\n",
    "    \n",
    "    # Step 3: Tokenize the text into words\n",
    "    tokenized_text = tokenize(lowercase_text)\n",
    "    \n",
    "    # Step 4: Remove stopwords from the tokenized text\n",
    "    processed_text = remove_stopwords(tokenized_text)\n",
    "    \n",
    "    return processed_text\n",
    "    \n",
    "    #### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BGOqa1mxKeA"
   },
   "source": [
    "## Question 3: Perform Preprocessing\n",
    "Now, we will read the corpus from the file created in Question 1. After that, we will call the preprocessing function for each document in the corpus.\n",
    "\n",
    "Hint: Call the `text_preprocessing()` function with `doc_content` as the input parameter and save the result in the variable `temp1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 3916/3916 [06:55<00:00,  9.42doc/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of clean_docs =  3916\n",
      "clean_docs[0]:\n",
      "ninh thuận_địa_điểm ưu_tiên nhà_máy điện hạt_nhân góc ninh_thuận , địa_điểm ưu_tiên lựa_chọn nhà_máy điện hạt_nhân vương_hữu , viện trưởng viện năng_lượng nguyên_tử việt_nam , địa_điểm hiện cân_nhắc lựa_chọn nhà_máy điện hạt_nhân ninh_thuận , phú_yên phan_rang tuy_nhiên , ninh_thuận địa_điểm ưu_tiên lựa_chọn tiến_triển dự_án xây_dựng nhà_máy điện hạt_nhân vn , vương_hữu , viện năng_lượng nguyên_tử việt_nam hoàn_tất dự_án tiền_khả_thi trình chính_phủ phê_duyệt dự_kiến , ( 2007 ) , dự_án khả_thi đáp_ứng kịp tiến_độ thời_gian xây_dựng nhà_máy điện hạt_nhân vn kế_hoạch , nhà_máy điện hạt_nhân đầu_tiên xây_dựng đi hoạt_động bắt_đầu 2020 quy_mô công_suất 2 000 mw 4 000 mw , chiếm 5 9 tổng_công_suất phát_điện toàn_quốc vấn_đề lựa_chọn công_nghệ nhà_máy điện hạt_nhân vn , viện năng_lượng nguyên_tử việt_nam , hiện quá_trình phân_tích trưng_cầu ý_kiến chuyên_gia tuy_nhiên , công_nghệ nhà_máy điện hạt_nhân chọn_lựa cơ_sở đảm_bảo an_toàn quá_trình vận_hành hướng_dẫn cơ_quan năng_lượng nguyên_tử quốc_tế ( iaea ) , nhà_máy điện hạt_nhân hoạt_động 3 500 4 500 , 500 700 trình_độ đại_học đại_học , 700 1 000 kỹ_thuật_viên 2 200 3 000 công_nhân lành_nghề hiện_nay , viện 681 cán_bộ trung_bình 42 , đại_học 361 , thạc_sỹ 78 , tiến_sỹ gs , pgs 62 tuổi_đời trung_bình cán_bộ viện năng_lượng nguyên_tử việt_nam 42 độ vn trẻ , quốc_tế già ! đại_diện khoa_học công_nghệ nhận_xét\n",
      "\n",
      "Document 1 (Original, First 100 chars):\n",
      "Ninh Thuận: Địa điểm ưu tiên đặt nhà máy điện hạt nhân Một góc Ninh Thuận, địa điểm ưu tiên lựa chọn\n",
      "Document 1 (Processed, First 100 chars):\n",
      "ninh thuận_địa_điểm ưu_tiên nhà_máy điện hạt_nhân góc ninh_thuận , địa_điểm ưu_tiên lựa_chọn nhà_máy\n",
      "\n",
      "Document 2 (Original, First 100 chars):\n",
      "Công nghệ nuôi tạo ngọc trai đen Viên ngọc trai đen to bằng hạt nhãn, có kích thước 15mm do ông Thiệ\n",
      "Document 2 (Processed, First 100 chars):\n",
      "công_nghệ nuôi ngọc_trai đen_viên ngọc_trai đen to hạt nhãn , kích_thước 15 mm thiện nuôi_cấy có_giá\n",
      "\n",
      "Document 3 (Original, First 100 chars):\n",
      "Hơn 16.000 loài có nguy cơ tuyệt chủng Loài vật đẹp đẽ này có thể biến mất hoàn toàn khỏi địa cầu tr\n",
      "Document 3 (Processed, First 100 chars):\n",
      "16 000 loài nguy_cơ tuyệt_chủng loài vật đẹp_đẽ có_thể biến hoàn_toàn địa_cầu tương_lai hiệp_hội bảo\n",
      "\n",
      "Document 4 (Original, First 100 chars):\n",
      "Nghiên cứu thành công thiết bị đun nước bằng năng lượng mặt trời Thiết bị đun nước bằng năng lượng m\n",
      "Document 4 (Processed, First 100 chars):\n",
      "nghiên_cứu thành_công thiết_bị đun năng_lượng mặt_trời thiết_bị đun năng_lượng mặt_trời 1 thực_hiện \n",
      "\n",
      "Document 5 (Original, First 100 chars):\n",
      "Tạo bộ kit phát hiện vi khuẩn bệnh thương hàn (NLĐ)- Thạc sĩ Phạm Thái Bình, Trung tâm Phát triển kh\n",
      "Document 5 (Processed, First 100 chars):\n",
      "kit phát_hiện vi_khuẩn bệnh thương_hàn ( nlđ ) thạc_sĩ phạm_thái_bình , trung_tâm phát_triển khoa_họ\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "clean_docs = []\n",
    "corpus_file = \"VNTC_khoahoc.txt\"\n",
    "\n",
    "# Precompute total lines\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    total_lines = sum(1 for _ in file)\n",
    "\n",
    "# Open the corpus file and preprocess each document with a progress bar\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in tqdm(file, desc=\"Processing documents\", unit=\"doc\", total=total_lines):\n",
    "        temp1 = text_preprocessing(line.strip())\n",
    "        clean_docs.append(temp1)\n",
    "\n",
    "print(\"\\nLength of clean_docs = \", len(clean_docs))\n",
    "print(\"clean_docs[0]:\\n\" + clean_docs[0])\n",
    "\n",
    "# Compare the first 100 characters of the original and processed documents\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    original_docs = [line.strip() for line in file]\n",
    "\n",
    "for i in range(min(len(original_docs), 5)):  # Display for the first 5 documents\n",
    "    print(f\"\\nDocument {i + 1} (Original, First 100 chars):\\n{original_docs[i][:100]}\")\n",
    "    print(f\"Document {i + 1} (Processed, First 100 chars):\\n{clean_docs[i][:100]}\")\n",
    "    \n",
    "\n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFhai6BwxKeB"
   },
   "source": [
    "## Question 4: Save Preprocessed Data\n",
    "Hint: Save the preprocessed data to a file named `dataset_name + '.clean.txt'`, where each document is written on a separate line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xfHmSiRrxKeB"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "output_file = f\"{dataset_name}.clean.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for doc in clean_docs:\n",
    "        out_file.write(doc + \"\\n\")\n",
    "\n",
    "#### YOUR CODE HERE ####"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
