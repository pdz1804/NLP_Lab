{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Exercise 1 on Machine Translation\n",
    "\n",
    "Implement a **sequence2sequence** model to translate English to Vietnamese. In this exercise, we will sequentially practice the steps to build a machine learning system for the machine translation task using a **seq2seq model**. These steps include:\n",
    "- Downloading and preprocessing bilingual data\n",
    "- Creating training data\n",
    "- Building a seq2seq model with **attention**\n",
    "- Visualizing attention data\n",
    "- Translating new sentences on real-world data.\n",
    "\n",
    "- **Data**: [IWSLTâ€™15 English-Vietnamese](https://www.manythings.org/anki/)  \n",
    "  - **Train set**: `train.en` and `train.vi`  \n",
    "  - **Validation set**: `tst2012.en` and `tst2012.vi`  \n",
    "  - **Test set**: `tst2013.en` and `tst2013.vi`\n",
    "\n",
    "**Note**: Submit only a **single Jupyter Notebook file** that can handle all tasks, including data downloading, preprocessing, model training, and model evaluation. *(Submissions that do not follow the guidelines will receive a score of 0.)*\n",
    "\n",
    "## Grading Criteria\n",
    "\n",
    "For valid submissions, scores will be assigned based on the **leaderboard ranking** (**strictly greater**):\n",
    "\n",
    "- **Top 25%** â†’ **10 points**\n",
    "- **25% - 50%** â†’ **9.0 points**\n",
    "- **50% - 75%** â†’ **8.0 points**\n",
    "- **75% - 100%** â†’ **7.0 points**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchtext in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: nltk in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: sacrebleu in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torchtext) (4.66.5)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: click in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: portalocker in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from portalocker->sacrebleu) (308)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->torchtext) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->torchtext) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->torchtext) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio torchtext nltk sacrebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Version: 2.5.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions\n",
      "Required-by: pgmpy, torchaudio, torchtext, torchvision\n",
      "---\n",
      "Name: torchtext\n",
      "Version: 0.18.0\n",
      "Summary: Text utilities, models, transforms, and datasets for PyTorch.\n",
      "Home-page: https://github.com/pytorch/text\n",
      "Author: PyTorch Text Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD\n",
      "Location: e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages\n",
      "Requires: numpy, requests, torch, tqdm\n",
      "Required-by: \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pip show torch torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sacrebleu\n",
    "\n",
    "nltk.download('punkt')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi.tar.gz\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi.tar.gz: 404 -- Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\file_utils.py:291\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 291\u001b[0m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    562\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m DATA_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Download dataset\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m path_to_zip \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miwslt15.en-vi.tar.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m path_to_data \u001b[38;5;241m=\u001b[39m path_to_zip\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miwslt15.en-vi.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miwslt15.en-vi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Read files\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\file_utils.py:293\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[0;32m    291\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39merrno, e\u001b[38;5;241m.\u001b[39mreason))\n",
      "\u001b[1;31mException\u001b[0m: URL fetch failure on https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi.tar.gz: 404 -- Not Found"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# ðŸ“¥ STEP 1: Download and Load Dataset\n",
    "# =====================================\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Download English-Vietnamese data\n",
    "urls = {\n",
    "    \"train.en\": \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\",\n",
    "    \"train.vi\": \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\",\n",
    "    \"tst2012.en\": \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2012.en\",\n",
    "    \"tst2012.vi\": \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2012.vi\",\n",
    "    \"tst2013.en\": \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\",\n",
    "    \"tst2013.vi\": \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi\",\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "for filename, url in urls.items():\n",
    "    filepath = os.path.join(\"data\", filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers for English & Vietnamese\n",
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "vi_tokenizer = get_tokenizer(\"moses\", language=\"vi\")\n",
    "\n",
    "# Read and tokenize sentences\n",
    "def yield_tokens(filepath, tokenizer):\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield tokenizer(line.strip())\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    return build_vocab_from_iterator(yield_tokens(filepath, tokenizer), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "en_vocab = build_vocab(\"data/train.en\", en_tokenizer)\n",
    "vi_vocab = build_vocab(\"data/train.vi\", vi_tokenizer)\n",
    "\n",
    "# Convert text to integer sequence\n",
    "def encode_sentence(sentence, vocab, tokenizer):\n",
    "    return [vocab[\"<bos>\"]] + [vocab[token] for token in tokenizer(sentence)] + [vocab[\"<eos>\"]]\n",
    "\n",
    "# Padding sequences\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "    return seq + [pad_idx] * (max_len - len(seq))\n",
    "\n",
    "# Dataset Class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, en_filepath, vi_filepath, en_vocab, vi_vocab, en_tokenizer, vi_tokenizer, max_len=50):\n",
    "        self.en_sentences = open(en_filepath, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "        self.vi_sentences = open(vi_filepath, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "        self.en_vocab = en_vocab\n",
    "        self.vi_vocab = vi_vocab\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.vi_tokenizer = vi_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_seq = encode_sentence(self.en_sentences[idx], self.en_vocab, self.en_tokenizer)\n",
    "        vi_seq = encode_sentence(self.vi_sentences[idx], self.vi_vocab, self.vi_tokenizer)\n",
    "        return torch.tensor(pad_sequence(en_seq, self.max_len, self.en_vocab[\"<pad>\"])), torch.tensor(pad_sequence(vi_seq, self.max_len, self.vi_vocab[\"<pad>\"]))\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = TranslationDataset(\"data/train.en\", \"data/train.vi\", en_vocab, vi_vocab, en_tokenizer, vi_tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.v(torch.tanh(self.attn(torch.cat((hidden.expand_as(encoder_outputs), encoder_outputs), dim=2))))\n",
    "        return torch.softmax(attn_energies.squeeze(2), dim=1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim + hidden_dim * 2, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 3, output_dim)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, trg, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(trg).unsqueeze(0)\n",
    "        attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs.permute(1, 0, 2)).squeeze(1)\n",
    "        rnn_input = torch.cat((embedded, context.unsqueeze(0)), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, torch.zeros_like(hidden)))\n",
    "        output = self.fc_out(torch.cat((output.squeeze(0), context), dim=1))\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(en_vocab), 256, 512).to(device)\n",
    "decoder = Decoder(len(vi_vocab), 256, 512).to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vi_vocab[\"<pad>\"])\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for en_batch, vi_batch in train_loader:\n",
    "        en_batch, vi_batch = en_batch.to(device), vi_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        encoder_outputs, hidden = encoder(en_batch)\n",
    "        loss = 0\n",
    "        for t in range(1, vi_batch.shape[1]):\n",
    "            output, hidden = decoder(vi_batch[:, t-1], hidden, encoder_outputs)\n",
    "            loss += criterion(output, vi_batch[:, t])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    en_seq = encode_sentence(sentence, en_vocab, en_tokenizer)\n",
    "    en_tensor = torch.tensor(pad_sequence(en_seq, 50, en_vocab[\"<pad>\"])).unsqueeze(0).to(device)\n",
    "    encoder_outputs, hidden = encoder(en_tensor)\n",
    "    vi_seq = [vi_vocab[\"<bos>\"]]\n",
    "    for _ in range(50):\n",
    "        output, hidden = decoder(torch.tensor([vi_seq[-1]]).to(device), hidden, encoder_outputs)\n",
    "        next_word = output.argmax(1).item()\n",
    "        if next_word == vi_vocab[\"<eos>\"]:\n",
    "            break\n",
    "        vi_seq.append(next_word)\n",
    "    return \" \".join([vi_vocab.lookup_token(w) for w in vi_seq])\n",
    "\n",
    "print(translate(\"Hello, how are you?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
