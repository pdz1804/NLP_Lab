{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Exercise 2 on Text Generation\n",
    "\n",
    "Implement a **sequence2sequence** model to **summarize the text**.\n",
    "\n",
    "- **Data**: [CNN-DailyMail News Text Summarization](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail)\n",
    "\n",
    "**Note**: Submit only a **single Jupyter Notebook file** that can handle all tasks, including data downloading, preprocessing, model training, and model evaluation. *(Submissions that do not follow the guidelines will receive a score of 0.)*\n",
    "\n",
    "## Grading Criteria\n",
    "\n",
    "For valid submissions, scores will be assigned based on the **leaderboard ranking** (**strictly greater**):\n",
    "\n",
    "- **Top 25%** ‚Üí **10 points**\n",
    "- **25% - 50%** ‚Üí **9.0 points**\n",
    "- **50% - 75%** ‚Üí **8.0 points**\n",
    "- **75% - 100%** ‚Üí **7.0 points**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow numpy pandas matplotlib tensorflow-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# =====================================\n",
    "# üì• STEP 1: Load CNN/DailyMail Dataset\n",
    "# =====================================\n",
    "# Load dataset using TensorFlow Datasets\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "config = \"3.0.0\"  # Use latest version\n",
    "dataset = tfds.load(name=dataset_name, split=[\"train\", \"validation\", \"test\"], as_supervised=True)\n",
    "\n",
    "# Convert dataset to NumPy arrays\n",
    "train_data, val_data, test_data = dataset\n",
    "\n",
    "def extract_data(dataset):\n",
    "    inputs, summaries = [], []\n",
    "    for text, summary in tfds.as_numpy(dataset):\n",
    "        inputs.append(text.decode('utf-8'))\n",
    "        summaries.append(summary.decode('utf-8'))\n",
    "    return inputs, summaries\n",
    "\n",
    "# Extract text and summaries\n",
    "train_texts, train_summaries = extract_data(train_data)\n",
    "val_texts, val_summaries = extract_data(val_data)\n",
    "test_texts, test_summaries = extract_data(test_data)\n",
    "\n",
    "# Display a sample\n",
    "print(f\"Example Text: {train_texts[0]}\")\n",
    "print(f\"Example Summary: {train_summaries[0]}\")\n",
    "\n",
    "# =====================================\n",
    "# üî¢ STEP 2: Tokenization & Preprocessing\n",
    "# =====================================\n",
    "# Define hyperparameters\n",
    "MAX_LEN_TEXT = 400\n",
    "MAX_LEN_SUMMARY = 100\n",
    "VOCAB_SIZE = 30000\n",
    "\n",
    "# Tokenize input texts\n",
    "def tokenize_sentences(sentences, vocab_size, max_length):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return tokenizer, sequences\n",
    "\n",
    "# Tokenize articles and summaries\n",
    "text_tokenizer, text_sequences = tokenize_sentences(train_texts, VOCAB_SIZE, MAX_LEN_TEXT)\n",
    "summary_tokenizer, summary_sequences = tokenize_sentences(train_summaries, VOCAB_SIZE, MAX_LEN_SUMMARY)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(text_sequences, summary_sequences, test_size=0.1, random_state=42)\n",
    "\n",
    "# =====================================\n",
    "# üèóÔ∏è STEP 3: Build Seq2Seq Model with Attention\n",
    "# =====================================\n",
    "# Define model parameters\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 512\n",
    "\n",
    "# Encoder Model\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(enc_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "        return output, state_h, state_c\n",
    "\n",
    "# Attention Layer\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Decoder Model with Attention\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state_h, state_c, attention_weights\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS, batch_sz=64)\n",
    "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS, batch_sz=64)\n",
    "\n",
    "# =====================================\n",
    "# üöÄ STEP 4: Train the Model\n",
    "# =====================================\n",
    "# Define optimizer and loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    enc_output, enc_hidden, enc_cell = encoder(X_train)\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    for i in range(len(y_train)):\n",
    "        dec_input = y_train[i]  # Get target sequence\n",
    "        predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "# =====================================\n",
    "# üìä STEP 5: Model Evaluation\n",
    "# =====================================\n",
    "# Translate test articles\n",
    "test_articles = [\"Breaking news: A major earthquake hit California\", \"New AI technology is revolutionizing healthcare\"]\n",
    "test_sequences = pad_sequences(text_tokenizer.texts_to_sequences(test_articles), maxlen=MAX_LEN_TEXT, padding=\"post\")\n",
    "enc_output, enc_hidden, enc_cell = encoder(test_sequences)\n",
    "summaries, _, _, _ = decoder(enc_output, enc_hidden, enc_output)\n",
    "\n",
    "print(\"Generated Summaries:\", summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
