{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2252621"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Exercise on Plagiarism Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this assignment, we will sequentially practice the steps to build a plagiarism detection application using a pre-trained Word2Vec model.\n",
    "\n",
    "- Data: https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c4147f9_data/data.zip\n",
    "- This exercise requires knowledge of Python programming with the following libraries:\n",
    "  - `gensim` (to load the Word2Vec model)\n",
    "  - `numpy` (to compute similarity)\n",
    "- Additionally, we will use a pre-trained Word2Vec model Google's pre-trained word2vec model: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
    "\n",
    "Steps to Solve This Exercise\n",
    "1. Exploring the Dataset  \n",
    "2. Build a class for computing document similarity (`DocSim` class)\n",
    "3. Create an instance of the above class (Load the pre-trained word embedding model, Create a list of stopwords and create an instance of the `DocSim` class)\n",
    "4. Plagiarism Detection and Evaluation\n",
    "\n",
    "**Note**: Submit only a single Jupyter Notebook file that can handle all tasks, including data downloading, preprocessing, and model training. (Submissions that do not follow the guidelines will receive a score of 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install gensim numpy requests tqdm zipfile36 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib as plt \n",
    "import gensim\n",
    "import sys\n",
    "import platform\n",
    "import gdown\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords dataset (only needed once)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable being used: e:\\anaconda3\\envs\\ml_env_test\\python.exe\n",
      "Python version: 3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]\n",
      "Operating System: Windows\n",
      "OS Version: 10.0.19045\n",
      "OS Release: 10\n",
      "Machine: AMD64\n",
      "Running in Visual Studio Code\n"
     ]
    }
   ],
   "source": [
    "# Python environment details\n",
    "print(\"Python executable being used:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "# Operating System details\n",
    "print(\"Operating System:\", platform.system())\n",
    "print(\"OS Version:\", platform.version())\n",
    "print(\"OS Release:\", platform.release())\n",
    "\n",
    "# Machine and architecture details\n",
    "print(\"Machine:\", platform.machine())\n",
    "\n",
    "# Visual Studio Code details (based on environment variable)\n",
    "vscode_info = os.environ.get('VSCODE_PID', None)\n",
    "if vscode_info:\n",
    "    print(\"Running in Visual Studio Code\")\n",
    "else:\n",
    "    print(\"Not running in Visual Studio Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"data\"):\n",
    "#     os.makedirs(\"data\")\n",
    "# if not os.path.exists(\"models\"):\n",
    "#     os.makedirs(\"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c4147f9_data/data.zip\"\n",
    "dataset_filename = \"data.zip\"\n",
    "\n",
    "if not os.path.exists(dataset_filename):\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(dataset_url)\n",
    "    with open(dataset_filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "\n",
    "    # Extract the dataset\n",
    "    print(\"Extracting dataset...\")\n",
    "    with ZipFile(dataset_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data\")\n",
    "    print(\"Dataset extracted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Word2Vec model already exists.\n"
     ]
    }
   ],
   "source": [
    "# Define download URL and file path\n",
    "# word2vec_url = \"https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\" # their links\n",
    "word2vec_url = \"https://drive.google.com/uc?id=1t6JI7xzGIh47O4-D19Ybs44zRB3dY5FX\" # our links\n",
    "word2vec_filename = \"models/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# Ensure the models directory exists\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# Check if the model file already exists\n",
    "if not os.path.exists(word2vec_filename):\n",
    "    print(\"Downloading pre-trained Word2Vec model (this may take a while)...\")\n",
    "    \n",
    "    try:\n",
    "        gdown.download(word2vec_url, word2vec_filename, quiet=False)\n",
    "        print(\"Pre-trained Word2Vec model downloaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading Word2Vec model: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Pre-trained Word2Vec model already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_directory_structure(root_dir=None, indent=\"\"):\n",
    "    \"\"\"\n",
    "    Prints the structure of the current working directory recursively.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): The directory to start from. If None, uses the current working directory.\n",
    "        indent (str): Indentation for subdirectories (used for recursion).\n",
    "    \"\"\"\n",
    "    if root_dir is None:\n",
    "        root_dir = os.getcwd()  # Use current directory if not specified\n",
    "\n",
    "    print(f\"ğŸ“‚ {root_dir}\")  # Print the root directory\n",
    "\n",
    "    for item in os.listdir(root_dir):\n",
    "        item_path = os.path.join(root_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{indent}ğŸ“ {item}\")  # Print folder name\n",
    "            print_directory_structure(item_path, indent + \"    \")  # Recursive call for subdirectory\n",
    "        else:\n",
    "            print(f\"{indent}ğŸ“„ {item}\")  # Print file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab05\n",
      "ğŸ“„ 2252621_Lab5_HEX copy.ipynb\n",
      "ğŸ“„ 2252621_Lab5_HEX.ipynb\n",
      "ğŸ“„ 2252621_Lab5_inclass.ipynb\n",
      "ğŸ“ data\n",
      "ğŸ“‚ e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab05\\data\n",
      "    ğŸ“ data\n",
      "ğŸ“‚ e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab05\\data\\data\n",
      "        ğŸ“„ .DS_Store\n",
      "        ğŸ“„ file_information.csv\n",
      "        ğŸ“„ g0pA_taska.txt\n",
      "        ğŸ“„ g0pA_taskb.txt\n",
      "        ğŸ“„ g0pA_taskc.txt\n",
      "        ğŸ“„ g0pA_taskd.txt\n",
      "        ğŸ“„ g0pA_taske.txt\n",
      "        ğŸ“„ g0pB_taska.txt\n",
      "        ğŸ“„ g0pB_taskb.txt\n",
      "        ğŸ“„ g0pB_taskc.txt\n",
      "        ğŸ“„ g0pB_taskd.txt\n",
      "        ğŸ“„ g0pB_taske.txt\n",
      "        ğŸ“„ g0pC_taska.txt\n",
      "        ğŸ“„ g0pC_taskb.txt\n",
      "        ğŸ“„ g0pC_taskc.txt\n",
      "        ğŸ“„ g0pC_taskd.txt\n",
      "        ğŸ“„ g0pC_taske.txt\n",
      "        ğŸ“„ g0pD_taska.txt\n",
      "        ğŸ“„ g0pD_taskb.txt\n",
      "        ğŸ“„ g0pD_taskc.txt\n",
      "        ğŸ“„ g0pD_taskd.txt\n",
      "        ğŸ“„ g0pD_taske.txt\n",
      "        ğŸ“„ g0pE_taska.txt\n",
      "        ğŸ“„ g0pE_taskb.txt\n",
      "        ğŸ“„ g0pE_taskc.txt\n",
      "        ğŸ“„ g0pE_taskd.txt\n",
      "        ğŸ“„ g0pE_taske.txt\n",
      "        ğŸ“„ g1pA_taska.txt\n",
      "        ğŸ“„ g1pA_taskb.txt\n",
      "        ğŸ“„ g1pA_taskc.txt\n",
      "        ğŸ“„ g1pA_taskd.txt\n",
      "        ğŸ“„ g1pA_taske.txt\n",
      "        ğŸ“„ g1pB_taska.txt\n",
      "        ğŸ“„ g1pB_taskb.txt\n",
      "        ğŸ“„ g1pB_taskc.txt\n",
      "        ğŸ“„ g1pB_taskd.txt\n",
      "        ğŸ“„ g1pB_taske.txt\n",
      "        ğŸ“„ g1pD_taska.txt\n",
      "        ğŸ“„ g1pD_taskb.txt\n",
      "        ğŸ“„ g1pD_taskc.txt\n",
      "        ğŸ“„ g1pD_taskd.txt\n",
      "        ğŸ“„ g1pD_taske.txt\n",
      "        ğŸ“„ g2pA_taska.txt\n",
      "        ğŸ“„ g2pA_taskb.txt\n",
      "        ğŸ“„ g2pA_taskc.txt\n",
      "        ğŸ“„ g2pA_taskd.txt\n",
      "        ğŸ“„ g2pA_taske.txt\n",
      "        ğŸ“„ g2pB_taska.txt\n",
      "        ğŸ“„ g2pB_taskb.txt\n",
      "        ğŸ“„ g2pB_taskc.txt\n",
      "        ğŸ“„ g2pB_taskd.txt\n",
      "        ğŸ“„ g2pB_taske.txt\n",
      "        ğŸ“„ g2pC_taska.txt\n",
      "        ğŸ“„ g2pC_taskb.txt\n",
      "        ğŸ“„ g2pC_taskc.txt\n",
      "        ğŸ“„ g2pC_taskd.txt\n",
      "        ğŸ“„ g2pC_taske.txt\n",
      "        ğŸ“„ g2pE_taska.txt\n",
      "        ğŸ“„ g2pE_taskb.txt\n",
      "        ğŸ“„ g2pE_taskc.txt\n",
      "        ğŸ“„ g2pE_taskd.txt\n",
      "        ğŸ“„ g2pE_taske.txt\n",
      "        ğŸ“„ g3pA_taska.txt\n",
      "        ğŸ“„ g3pA_taskb.txt\n",
      "        ğŸ“„ g3pA_taskc.txt\n",
      "        ğŸ“„ g3pA_taskd.txt\n",
      "        ğŸ“„ g3pA_taske.txt\n",
      "        ğŸ“„ g3pB_taska.txt\n",
      "        ğŸ“„ g3pB_taskb.txt\n",
      "        ğŸ“„ g3pB_taskc.txt\n",
      "        ğŸ“„ g3pB_taskd.txt\n",
      "        ğŸ“„ g3pB_taske.txt\n",
      "        ğŸ“„ g3pC_taska.txt\n",
      "        ğŸ“„ g3pC_taskb.txt\n",
      "        ğŸ“„ g3pC_taskc.txt\n",
      "        ğŸ“„ g3pC_taskd.txt\n",
      "        ğŸ“„ g3pC_taske.txt\n",
      "        ğŸ“„ g4pB_taska.txt\n",
      "        ğŸ“„ g4pB_taskb.txt\n",
      "        ğŸ“„ g4pB_taskc.txt\n",
      "        ğŸ“„ g4pB_taskd.txt\n",
      "        ğŸ“„ g4pB_taske.txt\n",
      "        ğŸ“„ g4pC_taska.txt\n",
      "        ğŸ“„ g4pC_taskb.txt\n",
      "        ğŸ“„ g4pC_taskc.txt\n",
      "        ğŸ“„ g4pC_taskd.txt\n",
      "        ğŸ“„ g4pC_taske.txt\n",
      "        ğŸ“„ g4pD_taska.txt\n",
      "        ğŸ“„ g4pD_taskb.txt\n",
      "        ğŸ“„ g4pD_taskc.txt\n",
      "        ğŸ“„ g4pD_taskd.txt\n",
      "        ğŸ“„ g4pD_taske.txt\n",
      "        ğŸ“„ g4pE_taska.txt\n",
      "        ğŸ“„ g4pE_taskb.txt\n",
      "        ğŸ“„ g4pE_taskc.txt\n",
      "        ğŸ“„ g4pE_taskd.txt\n",
      "        ğŸ“„ g4pE_taske.txt\n",
      "        ğŸ“„ orig_taska.txt\n",
      "        ğŸ“„ orig_taskb.txt\n",
      "        ğŸ“„ orig_taskc.txt\n",
      "        ğŸ“„ orig_taskd.txt\n",
      "        ğŸ“„ orig_taske.txt\n",
      "        ğŸ“„ test_info.csv\n",
      "    ğŸ“ __MACOSX\n",
      "ğŸ“‚ e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab05\\data\\__MACOSX\n",
      "        ğŸ“„ ._data\n",
      "        ğŸ“ data\n",
      "ğŸ“‚ e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab05\\data\\__MACOSX\\data\n",
      "            ğŸ“„ ._.DS_Store\n",
      "            ğŸ“„ ._file_information.csv\n",
      "            ğŸ“„ ._g0pA_taska.txt\n",
      "            ğŸ“„ ._g0pA_taskb.txt\n",
      "            ğŸ“„ ._g0pA_taskc.txt\n",
      "            ğŸ“„ ._g0pA_taskd.txt\n",
      "            ğŸ“„ ._g0pA_taske.txt\n",
      "            ğŸ“„ ._g0pB_taska.txt\n",
      "            ğŸ“„ ._g0pB_taskb.txt\n",
      "            ğŸ“„ ._g0pB_taskc.txt\n",
      "            ğŸ“„ ._g0pB_taskd.txt\n",
      "            ğŸ“„ ._g0pB_taske.txt\n",
      "            ğŸ“„ ._g0pC_taska.txt\n",
      "            ğŸ“„ ._g0pC_taskb.txt\n",
      "            ğŸ“„ ._g0pC_taskc.txt\n",
      "            ğŸ“„ ._g0pC_taskd.txt\n",
      "            ğŸ“„ ._g0pC_taske.txt\n",
      "            ğŸ“„ ._g0pD_taska.txt\n",
      "            ğŸ“„ ._g0pD_taskb.txt\n",
      "            ğŸ“„ ._g0pD_taskc.txt\n",
      "            ğŸ“„ ._g0pD_taskd.txt\n",
      "            ğŸ“„ ._g0pD_taske.txt\n",
      "            ğŸ“„ ._g0pE_taska.txt\n",
      "            ğŸ“„ ._g0pE_taskb.txt\n",
      "            ğŸ“„ ._g0pE_taskc.txt\n",
      "            ğŸ“„ ._g0pE_taskd.txt\n",
      "            ğŸ“„ ._g0pE_taske.txt\n",
      "            ğŸ“„ ._g1pA_taska.txt\n",
      "            ğŸ“„ ._g1pA_taskb.txt\n",
      "            ğŸ“„ ._g1pA_taskc.txt\n",
      "            ğŸ“„ ._g1pA_taskd.txt\n",
      "            ğŸ“„ ._g1pA_taske.txt\n",
      "            ğŸ“„ ._g1pB_taska.txt\n",
      "            ğŸ“„ ._g1pB_taskb.txt\n",
      "            ğŸ“„ ._g1pB_taskc.txt\n",
      "            ğŸ“„ ._g1pB_taskd.txt\n",
      "            ğŸ“„ ._g1pB_taske.txt\n",
      "            ğŸ“„ ._g1pD_taska.txt\n",
      "            ğŸ“„ ._g1pD_taskb.txt\n",
      "            ğŸ“„ ._g1pD_taskc.txt\n",
      "            ğŸ“„ ._g1pD_taskd.txt\n",
      "            ğŸ“„ ._g1pD_taske.txt\n",
      "            ğŸ“„ ._g2pA_taska.txt\n",
      "            ğŸ“„ ._g2pA_taskb.txt\n",
      "            ğŸ“„ ._g2pA_taskc.txt\n",
      "            ğŸ“„ ._g2pA_taskd.txt\n",
      "            ğŸ“„ ._g2pA_taske.txt\n",
      "            ğŸ“„ ._g2pB_taska.txt\n",
      "            ğŸ“„ ._g2pB_taskb.txt\n",
      "            ğŸ“„ ._g2pB_taskc.txt\n",
      "            ğŸ“„ ._g2pB_taskd.txt\n",
      "            ğŸ“„ ._g2pB_taske.txt\n",
      "            ğŸ“„ ._g2pC_taska.txt\n",
      "            ğŸ“„ ._g2pC_taskb.txt\n",
      "            ğŸ“„ ._g2pC_taskc.txt\n",
      "            ğŸ“„ ._g2pC_taskd.txt\n",
      "            ğŸ“„ ._g2pC_taske.txt\n",
      "            ğŸ“„ ._g2pE_taska.txt\n",
      "            ğŸ“„ ._g2pE_taskb.txt\n",
      "            ğŸ“„ ._g2pE_taskc.txt\n",
      "            ğŸ“„ ._g2pE_taskd.txt\n",
      "            ğŸ“„ ._g2pE_taske.txt\n",
      "            ğŸ“„ ._g3pA_taska.txt\n",
      "            ğŸ“„ ._g3pA_taskb.txt\n",
      "            ğŸ“„ ._g3pA_taskc.txt\n",
      "            ğŸ“„ ._g3pA_taskd.txt\n",
      "            ğŸ“„ ._g3pA_taske.txt\n",
      "            ğŸ“„ ._g3pB_taska.txt\n",
      "            ğŸ“„ ._g3pB_taskb.txt\n",
      "            ğŸ“„ ._g3pB_taskc.txt\n",
      "            ğŸ“„ ._g3pB_taskd.txt\n",
      "            ğŸ“„ ._g3pB_taske.txt\n",
      "            ğŸ“„ ._g3pC_taska.txt\n",
      "            ğŸ“„ ._g3pC_taskb.txt\n",
      "            ğŸ“„ ._g3pC_taskc.txt\n",
      "            ğŸ“„ ._g3pC_taskd.txt\n",
      "            ğŸ“„ ._g3pC_taske.txt\n",
      "            ğŸ“„ ._g4pB_taska.txt\n",
      "            ğŸ“„ ._g4pB_taskb.txt\n",
      "            ğŸ“„ ._g4pB_taskc.txt\n",
      "            ğŸ“„ ._g4pB_taskd.txt\n",
      "            ğŸ“„ ._g4pB_taske.txt\n",
      "            ğŸ“„ ._g4pC_taska.txt\n",
      "            ğŸ“„ ._g4pC_taskb.txt\n",
      "            ğŸ“„ ._g4pC_taskc.txt\n",
      "            ğŸ“„ ._g4pC_taskd.txt\n",
      "            ğŸ“„ ._g4pC_taske.txt\n",
      "            ğŸ“„ ._g4pD_taska.txt\n",
      "            ğŸ“„ ._g4pD_taskb.txt\n",
      "            ğŸ“„ ._g4pD_taskc.txt\n",
      "            ğŸ“„ ._g4pD_taskd.txt\n",
      "            ğŸ“„ ._g4pD_taske.txt\n",
      "            ğŸ“„ ._g4pE_taska.txt\n",
      "            ğŸ“„ ._g4pE_taskb.txt\n",
      "            ğŸ“„ ._g4pE_taskc.txt\n",
      "            ğŸ“„ ._g4pE_taskd.txt\n",
      "            ğŸ“„ ._g4pE_taske.txt\n",
      "            ğŸ“„ ._orig_taska.txt\n",
      "            ğŸ“„ ._orig_taskc.txt\n",
      "            ğŸ“„ ._orig_taskd.txt\n",
      "            ğŸ“„ ._orig_taske.txt\n",
      "            ğŸ“„ ._test_info.csv\n",
      "ğŸ“„ data.zip\n",
      "ğŸ“ datasets\n",
      "ğŸ“‚ e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab05\\datasets\n",
      "    ğŸ“„ Online Retail.xlsx\n",
      "ğŸ“ models\n",
      "ğŸ“‚ e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3085_NLP\\BT\\Lab05\\models\n",
      "    ğŸ“„ GoogleNews-vectors-negative300.bin.gz\n",
      "ğŸ“„ train_similarities.csv\n"
     ]
    }
   ],
   "source": [
    "print_directory_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset directory:\n",
      "['.DS_Store', 'file_information.csv', 'g0pA_taska.txt', 'g0pA_taskb.txt', 'g0pA_taskc.txt', 'g0pA_taskd.txt', 'g0pA_taske.txt', 'g0pB_taska.txt', 'g0pB_taskb.txt', 'g0pB_taskc.txt', 'g0pB_taskd.txt', 'g0pB_taske.txt', 'g0pC_taska.txt', 'g0pC_taskb.txt', 'g0pC_taskc.txt', 'g0pC_taskd.txt', 'g0pC_taske.txt', 'g0pD_taska.txt', 'g0pD_taskb.txt', 'g0pD_taskc.txt', 'g0pD_taskd.txt', 'g0pD_taske.txt', 'g0pE_taska.txt', 'g0pE_taskb.txt', 'g0pE_taskc.txt', 'g0pE_taskd.txt', 'g0pE_taske.txt', 'g1pA_taska.txt', 'g1pA_taskb.txt', 'g1pA_taskc.txt', 'g1pA_taskd.txt', 'g1pA_taske.txt', 'g1pB_taska.txt', 'g1pB_taskb.txt', 'g1pB_taskc.txt', 'g1pB_taskd.txt', 'g1pB_taske.txt', 'g1pD_taska.txt', 'g1pD_taskb.txt', 'g1pD_taskc.txt', 'g1pD_taskd.txt', 'g1pD_taske.txt', 'g2pA_taska.txt', 'g2pA_taskb.txt', 'g2pA_taskc.txt', 'g2pA_taskd.txt', 'g2pA_taske.txt', 'g2pB_taska.txt', 'g2pB_taskb.txt', 'g2pB_taskc.txt', 'g2pB_taskd.txt', 'g2pB_taske.txt', 'g2pC_taska.txt', 'g2pC_taskb.txt', 'g2pC_taskc.txt', 'g2pC_taskd.txt', 'g2pC_taske.txt', 'g2pE_taska.txt', 'g2pE_taskb.txt', 'g2pE_taskc.txt', 'g2pE_taskd.txt', 'g2pE_taske.txt', 'g3pA_taska.txt', 'g3pA_taskb.txt', 'g3pA_taskc.txt', 'g3pA_taskd.txt', 'g3pA_taske.txt', 'g3pB_taska.txt', 'g3pB_taskb.txt', 'g3pB_taskc.txt', 'g3pB_taskd.txt', 'g3pB_taske.txt', 'g3pC_taska.txt', 'g3pC_taskb.txt', 'g3pC_taskc.txt', 'g3pC_taskd.txt', 'g3pC_taske.txt', 'g4pB_taska.txt', 'g4pB_taskb.txt', 'g4pB_taskc.txt', 'g4pB_taskd.txt', 'g4pB_taske.txt', 'g4pC_taska.txt', 'g4pC_taskb.txt', 'g4pC_taskc.txt', 'g4pC_taskd.txt', 'g4pC_taske.txt', 'g4pD_taska.txt', 'g4pD_taskb.txt', 'g4pD_taskc.txt', 'g4pD_taskd.txt', 'g4pD_taske.txt', 'g4pE_taska.txt', 'g4pE_taskb.txt', 'g4pE_taskc.txt', 'g4pE_taskd.txt', 'g4pE_taske.txt', 'orig_taska.txt', 'orig_taskb.txt', 'orig_taskc.txt', 'orig_taskd.txt', 'orig_taske.txt', 'test_info.csv']\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "data_path = \"data/data\"\n",
    "print(\"Files in dataset directory:\")\n",
    "print(os.listdir(data_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset path\n",
    "DATA_PATH = \"data/data\"\n",
    "CSV_FILE = os.path.join(DATA_PATH, \"file_information.csv\")\n",
    "\n",
    "# Load file classification information (train data only)\n",
    "df_info = pd.read_csv(CSV_FILE)\n",
    "df_info.columns = [\"File\", \"Task\", \"Category\"]  # Rename columns for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Task</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>a</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>b</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>c</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>d</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>e</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Task Category\n",
       "0  g0pA_taska.txt    a      non\n",
       "1  g0pA_taskb.txt    b      cut\n",
       "2  g0pA_taskc.txt    c    light\n",
       "3  g0pA_taskd.txt    d    heavy\n",
       "4  g0pA_taske.txt    e      non"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a class for computing document similarity (```DocSim``` class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocSim:\n",
    "    def __init__(self, model, stopwords=None):\n",
    "        \"\"\"\n",
    "        Initialize the DocSim class with a pre-trained word embedding model and stopwords.\n",
    "        \n",
    "        Args:\n",
    "            model: A pre-trained word embedding model (e.g., Word2Vec, FastText).\n",
    "            stopwords: A list of stopwords to ignore during similarity calculation.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.stopwords = stopwords if stopwords else []\n",
    "\n",
    "    def _vectorize(self, words):\n",
    "        \"\"\"\n",
    "        Vectorize a list of words using the word embedding model.\n",
    "        \n",
    "        Args:\n",
    "            words: A list of words to vectorize.\n",
    "        \n",
    "        Returns:\n",
    "            The average vector of the words.\n",
    "        \"\"\"\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            if word not in self.stopwords and word in self.model:\n",
    "                vectors.append(self.model[word])\n",
    "        \n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.model.vector_size)\n",
    "    \n",
    "    def similarity(self, source, target):\n",
    "        \"\"\"\n",
    "        Calculate the similarity between two documents.\n",
    "        \n",
    "        Args:\n",
    "            source: Source document (string).\n",
    "            target: Target document (string).\n",
    "        \n",
    "        Returns:\n",
    "            Cosine similarity score.\n",
    "        \"\"\"\n",
    "        source_vector = self._vectorize(source.split())\n",
    "        target_vector = self._vectorize(target.split())\n",
    "        return np.dot(source_vector, target_vector) / (\n",
    "            np.linalg.norm(source_vector) * np.linalg.norm(target_vector)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of the above class \n",
    "\n",
    "- Load the pre-trained word embedding model \n",
    "- Create a list of stopwords and create an instance of the ```DocSim``` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Word2Vec model...\n",
      "Word2Vec model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Google's pre-trained Word2Vec model\n",
    "print(\"Loading pre-trained Word2Vec model...\")\n",
    "word2vec_path = \"models/GoogleNews-vectors-negative300.bin.gz\"\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "print(\"Word2Vec model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Stopwords: ['no', 'very', 'was', 'has', 'wouldn', 'between', 't', \"hasn't\", 'few', 'his']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of common English stopwords\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Example Stopwords:\", list(stopwords_list)[:10])  # Show a few stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_stopwords = {\n",
    "#     \"amazon\", \"product\", \"review\", \"music\", \"album\", \"track\", \"song\", \"listen\",\n",
    "#     \"buy\", \"purchased\", \"download\", \"play\", \"sound\", \"favorite\", \"cd\", \"mp3\",\n",
    "#     \"great\", \"good\", \"best\", \"bad\", \"worst\"\n",
    "# }\n",
    "\n",
    "# # Merge with NLTK stopwords\n",
    "# stopwords_list.update(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DocSim class\n",
    "doc_sim = DocSim(model, stopwords=stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plagiarism Detection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: g1pB_taska.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g1pB_taskb.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g1pB_taskd.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g2pA_taska.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g2pA_taskb.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g2pB_taska.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g2pB_taskb.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g2pB_taskc.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g3pA_taska.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g4pB_taskb.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g4pB_taskd.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g4pB_taske.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g4pD_taskd.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g4pD_taske.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g4pE_taskb.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g4pE_taskc.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n",
      "Warning: g4pE_taskd.txt has non-UTF-8 encoding. Trying ISO-8859-1.\n"
     ]
    }
   ],
   "source": [
    "# Load all text files and map them to their category\n",
    "text_data = {}\n",
    "for file_name in df_info[\"File\"]:\n",
    "    file_path = os.path.join(DATA_PATH, file_name)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_data[file_name] = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Warning: {file_name} has non-UTF-8 encoding. Trying ISO-8859-1.\")\n",
    "        with open(file_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "            text_data[file_name] = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['g0pA_taska.txt', 'g0pA_taskb.txt', 'g0pA_taskc.txt', 'g0pA_taskd.txt', 'g0pA_taske.txt', 'g0pB_taska.txt', 'g0pB_taskb.txt', 'g0pB_taskc.txt', 'g0pB_taskd.txt', 'g0pB_taske.txt', 'g0pC_taska.txt', 'g0pC_taskb.txt', 'g0pC_taskc.txt', 'g0pC_taskd.txt', 'g0pC_taske.txt', 'g0pD_taska.txt', 'g0pD_taskb.txt', 'g0pD_taskc.txt', 'g0pD_taskd.txt', 'g0pD_taske.txt', 'g0pE_taska.txt', 'g0pE_taskb.txt', 'g0pE_taskc.txt', 'g0pE_taskd.txt', 'g0pE_taske.txt', 'g1pA_taska.txt', 'g1pA_taskb.txt', 'g1pA_taskc.txt', 'g1pA_taskd.txt', 'g1pA_taske.txt', 'g1pB_taska.txt', 'g1pB_taskb.txt', 'g1pB_taskc.txt', 'g1pB_taskd.txt', 'g1pB_taske.txt', 'g1pD_taska.txt', 'g1pD_taskb.txt', 'g1pD_taskc.txt', 'g1pD_taskd.txt', 'g1pD_taske.txt', 'g2pA_taska.txt', 'g2pA_taskb.txt', 'g2pA_taskc.txt', 'g2pA_taskd.txt', 'g2pA_taske.txt', 'g2pB_taska.txt', 'g2pB_taskb.txt', 'g2pB_taskc.txt', 'g2pB_taskd.txt', 'g2pB_taske.txt', 'g2pC_taska.txt', 'g2pC_taskb.txt', 'g2pC_taskc.txt', 'g2pC_taskd.txt', 'g2pC_taske.txt', 'g2pE_taska.txt', 'g2pE_taskb.txt', 'g2pE_taskc.txt', 'g2pE_taskd.txt', 'g2pE_taske.txt', 'g3pA_taska.txt', 'g3pA_taskb.txt', 'g3pA_taskc.txt', 'g3pA_taskd.txt', 'g3pA_taske.txt', 'g3pB_taska.txt', 'g3pB_taskb.txt', 'g3pB_taskc.txt', 'g3pB_taskd.txt', 'g3pB_taske.txt', 'g3pC_taska.txt', 'g3pC_taskb.txt', 'g3pC_taskc.txt', 'g3pC_taskd.txt', 'g3pC_taske.txt', 'g4pB_taska.txt', 'g4pB_taskb.txt', 'g4pB_taskc.txt', 'g4pB_taskd.txt', 'g4pB_taske.txt', 'g4pC_taska.txt', 'g4pC_taskb.txt', 'g4pC_taskc.txt', 'g4pC_taskd.txt', 'g4pC_taske.txt', 'g4pD_taska.txt', 'g4pD_taskb.txt', 'g4pD_taskc.txt', 'g4pD_taskd.txt', 'g4pD_taske.txt', 'g4pE_taska.txt', 'g4pE_taskb.txt', 'g4pE_taskc.txt', 'g4pE_taskd.txt', 'g4pE_taske.txt', 'orig_taska.txt', 'orig_taskb.txt', 'orig_taskc.txt', 'orig_taskd.txt', 'orig_taske.txt'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inheritance is a basic concept of Object-Oriented Programming where\\nthe basic idea is to create new classes that add extra detail to\\nexisting classes. This is done by allowing the new classes to reuse\\nthe methods and variables of the existing classes and new methods and\\nclasses are added to specialise the new class. Inheritance models the\\nâ€œis-kind-ofâ€ relationship between entities (or objects), for example,\\npostgraduates and undergraduates are both kinds of student. This kind\\nof relationship can be visualised as a tree structure, where â€˜studentâ€™\\nwould be the more general root node and both â€˜postgraduateâ€™ and\\nâ€˜undergraduateâ€™ would be more specialised extensions of the â€˜studentâ€™\\nnode (or the child nodes). In this relationship â€˜studentâ€™ would be\\nknown as the superclass or parent class whereas, â€˜postgraduateâ€™ would\\nbe known as the subclass or child class because the â€˜postgraduateâ€™\\nclass extends the â€˜studentâ€™ class.\\n\\nInheritance can occur on several layers, where if visualised would\\ndisplay a larger tree structure. For example, we could further extend\\nthe â€˜postgraduateâ€™ node by adding two extra extended classes to it\\ncalled, â€˜MSc Studentâ€™ and â€˜PhD Studentâ€™ as both these types of student\\nare kinds of postgraduate student. This would mean that both the â€˜MSc\\nStudentâ€™ and â€˜PhD Studentâ€™ classes would inherit methods and variables\\nfrom both the â€˜postgraduateâ€™ and â€˜student classesâ€™.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data['g0pA_taska.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   File      100 non-null    object\n",
      " 1   Task      100 non-null    object\n",
      " 2   Category  100 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.5+ KB\n",
      "None\n",
      "['non' 'cut' 'light' 'heavy' 'orig']\n"
     ]
    }
   ],
   "source": [
    "print(df_info.info())\n",
    "print(df_info[\"Category\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing document similarities...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training similarities saved to train_similarities.csv (sorted, no duplicates)\n"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "results = set()  # Use a set to store unique comparisons\n",
    "remove_set = set() # Remove\n",
    "\n",
    "# Compare each document with others (training only, no test data)\n",
    "print(\"Computing document similarities...\")\n",
    "for file1, text1 in text_data.items():\n",
    "    for file2, text2 in text_data.items():\n",
    "        if file1 != file2:  # Avoid self-comparison\n",
    "            if (file1, file2) not in remove_set and (file2, file1) not in remove_set:\n",
    "                remove_set.add((file1, file2))\n",
    "                similarity = doc_sim.similarity(text1, text2)\n",
    "                category1 = df_info[df_info[\"File\"] == file1][\"Category\"].values[0]\n",
    "                category2 = df_info[df_info[\"File\"] == file2][\"Category\"].values[0]\n",
    "\n",
    "                # Store unique pair with categories\n",
    "                results.add((file1, file2, similarity, category1, category2))\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"File1\", \"File2\", \"Similarity\", \"Category1\", \"Category2\"])\n",
    "\n",
    "# Sort by Similarity (Descending)\n",
    "df_results = df_results.sort_values(by=\"Similarity\", ascending=False)\n",
    "\n",
    "# Save sorted results\n",
    "df_results.to_csv(\"train_similarities.csv\", index=False)\n",
    "print(\"Training similarities saved to train_similarities.csv (sorted, no duplicates)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4950 entries, 1348 to 1798\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   File1       4950 non-null   object \n",
      " 1   File2       4950 non-null   object \n",
      " 2   Similarity  4950 non-null   float32\n",
      " 3   Category1   4950 non-null   object \n",
      " 4   Category2   4950 non-null   object \n",
      "dtypes: float32(1), object(4)\n",
      "memory usage: 212.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File1</th>\n",
       "      <th>File2</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Category1</th>\n",
       "      <th>Category2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>g0pE_taska.txt</td>\n",
       "      <td>orig_taska.txt</td>\n",
       "      <td>0.997738</td>\n",
       "      <td>light</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>g3pA_taskd.txt</td>\n",
       "      <td>orig_taskd.txt</td>\n",
       "      <td>0.997639</td>\n",
       "      <td>cut</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>g4pC_taska.txt</td>\n",
       "      <td>orig_taska.txt</td>\n",
       "      <td>0.997398</td>\n",
       "      <td>cut</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>g4pC_taskd.txt</td>\n",
       "      <td>orig_taskd.txt</td>\n",
       "      <td>0.996071</td>\n",
       "      <td>heavy</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>g3pA_taskd.txt</td>\n",
       "      <td>g4pC_taskd.txt</td>\n",
       "      <td>0.994818</td>\n",
       "      <td>cut</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>g0pE_taska.txt</td>\n",
       "      <td>g4pC_taska.txt</td>\n",
       "      <td>0.994622</td>\n",
       "      <td>light</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>g2pA_taskc.txt</td>\n",
       "      <td>orig_taskc.txt</td>\n",
       "      <td>0.994138</td>\n",
       "      <td>light</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>g2pB_taskd.txt</td>\n",
       "      <td>g3pA_taskd.txt</td>\n",
       "      <td>0.989713</td>\n",
       "      <td>light</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070</th>\n",
       "      <td>g2pB_taskd.txt</td>\n",
       "      <td>orig_taskd.txt</td>\n",
       "      <td>0.989347</td>\n",
       "      <td>light</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>g0pB_taskc.txt</td>\n",
       "      <td>orig_taskc.txt</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>cut</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               File1           File2  Similarity Category1 Category2\n",
       "1348  g0pE_taska.txt  orig_taska.txt    0.997738     light      orig\n",
       "4611  g3pA_taskd.txt  orig_taskd.txt    0.997639       cut      orig\n",
       "1783  g4pC_taska.txt  orig_taska.txt    0.997398       cut      orig\n",
       "2283  g4pC_taskd.txt  orig_taskd.txt    0.996071     heavy      orig\n",
       "4172  g3pA_taskd.txt  g4pC_taskd.txt    0.994818       cut     heavy\n",
       "559   g0pE_taska.txt  g4pC_taska.txt    0.994622     light       cut\n",
       "4374  g2pA_taskc.txt  orig_taskc.txt    0.994138     light      orig\n",
       "736   g2pB_taskd.txt  g3pA_taskd.txt    0.989713     light       cut\n",
       "4070  g2pB_taskd.txt  orig_taskd.txt    0.989347     light      orig\n",
       "2382  g0pB_taskc.txt  orig_taskc.txt    0.988685       cut      orig"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
